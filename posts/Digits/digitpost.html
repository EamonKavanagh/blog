<p>
Computerized digit recognition is an import and interesting introduction to the world
of machine learning.  Imagine the task given to the United States Postal Service- sort and
deliver 177 billion pieces of mail annually.  I'm sure in years past (presumably when 
the sheer volume of mail was much less) this task was handled by people carefully
reading envelopes and packages, sorting and delivering them through sheer force of will.
Now the postal service automates much of the task.  This has, in part, been made easy with 
the advent of printed labels and barcodes that specify all of the necessary information in
an easily readable format.  Standard handwritten letters still abound, though, and despite
the difficulty of having a computer read someone's chicken scratch, this is also mostly
automated.  Interestingly, if a letter is flagged as unreadable, a local postal employee
attempts to read the handwriting.  If they cannot figure it out, the letter may
get sent to the Mail Recovery Center in Atlanta (formerly the Dead Letter Office) where
the letter will be opened to search for clues.  It's heartwarming to know that someone out 
there is spending a great amount of effort trying to determine a lost letter's recipient.
</p>

<p>
The first step in the automation process is figuring out where the lines of text are on the
envelope and splitting up each line into individual words and each word into individual 
characters.  This can be done with a technique known as the sliding window.  A digitized 
image is scanned by a computer from left to right, top to bottom in rectangular boxes
of a predetermined size.  The process resembles a typewriter moving a page
along its carriage.  Essentially, you're subdividing the image into subimages and checking 
to see if any text is in the given frame.  The computer decides whether a subimage contains a 
block of text, word, or character (or possibly nothing) by applying an algorithm 
(say, logistic regression) to the pixels of the 
image.  I won't discuss this topic in much detail but it's very similar in flavor to the
top at hand.
</p>

<div class="post-image">
<img src="/static/slidingwindow.gif"/>
</div>

<p>
The aspect of computerized address reading I'll be investigating is digit classification.  I'm
assuming that the text on the envelope has been properly located and segmented and I receive uniformly sized
images containing a single digit (0-9).  Since this is a popular introductory machine learning problem
there are many datasets containing exactly that- collections of labeled digits that 
you may split into a training set to build the algorithm and a testing set to see how well
it performs.
The dataset I'll be looking at is from MNIST (Mixed National Institute of Standards 
and Technology).  The dataset is comprised of 60,000 images that are 28x28 
pixels.  Each image is in grayscale so the pixel values range from 0 (black) to 255 (white).
What this means is that a digit is represented by 784 (\(28^2\)) numbers that indicate 
the presence and intensity of ink in a 1x1 pixel box.  A single image is represetend
by a matrix of these numbers as seen below.
</p>

<div class="post-image">
<img src="/static/siximage.png"/>
</div>

<p>
The matrix is then flattened into a long vector with an understood ordering of left to right
and top to bottom; the first 28 numbers in the vector are the first row of pixels, the second
28 the second row and so on.  Once you understand the numerical representation of an image, 
you can begin thinking about how you would determine which digit an unlabeled image
might be.  When I first read about the digit classification problem I gravitated towards
a solution that hinged upon the idea that all digits with the same label will have very
similar visual and, as a result, numerical structure.  If the numerical
structure is indeed similar among labeled groups then all you needs to do is determine
which group an unlabeled example is most similar too.
</p>

<p>
How are we going to determine similarity?  The easiest way is with a metric, or measure
of distance, between two images.  Since we already have the numerical representation in
convenient vector form this is an easy and well understood problem.  There are many choices 
of metrics available for real numbers, such as looking at the sum of the absolute value of the 
difference between two vectors (\( l_1 \) norm).  Another similar and more common choice
would be the Euclidean (\( l_2 \)) norm where distance is measured as the sum of the 
squared difference.  Note that these are both understood to be component wise.

\[ \text{distance}(U,V) = \|U-V\| = \sqrt{\sum_{i = 1}^N (U_i - V_i)^2} \]
</p>

<p>
For this method it's as easy as calculating the distance to all the labeled digits with
the above formula and finding the minimum value.  This leads to a very short and easy
Python script.
</p>

<br>
<pre class="prettyprint linenums">
from random import shuffle

import numpy as np

print "Reading data"
data = np.loadtxt("train.csv", delimiter=",")
print "Data loaded"
shuffle(data)

numTrain = 4000
trainLabels = np.asarray([x[0] for x in data[:numTrain]])
train = np.asarray([x[1:] for x in data[:numTrain]])
testLabels = np.asarray([x[0] for x in data[numTrain:]])
test = np.asarray([x[1:] for x in data[numTrain:]])
predictions = []


for image in test:
    index = []
    dist = 1000000
    for i in xrange(len(train)):
        newDist = np.linalg.norm(image-train[i])
        if newDist < dist:
            dist = newDist
            index = i
    predictions.append(trainLabels[index])
    
print float(sum(predictions==testLabels))/len(predictions)
</pre>
<br>

<p>
The data is read in and randomly shuffled to make sure the initial order
doesn't bias the results.  The set is then partitioned into a training set of 4000 images
and a testing set of 1001 and the labels are also separated from the pixel values.  We
loop through all the test set images and for each test set image all of those in the 
training set to find the one with the minimal distance and attach the label of that 
image to the test image.  Since we have the luxury of knowing the actual label each image
in the testing set has we can see how well the method does.  For this fairly small training
set the method actual does surprising well, getting ~93% of them correct on any given run.
We can even take a look at a few of the images it got wrong and which one was the closest.
</p>

<div class="post-image">
<img src="/static/incorrectlabels.png"/>
</div>

<p>
The images on the left are the incorrectly classified digit and next to them are their
closest match in the set.  You can kind of see why the algorithm  labeled 
these incorrectly.  There are a few ways around this.  Instead of finding
the single training set example with the smallest distance, we could measure each image 
against all of the label groups and compute the average distance from the group
or perhaps the range.  The label could then be chosen from the group with the smallest
quantity.
</p>

<p>
One additional concern is runtime.  For this fairly small training and testing set the 
computation takes a fair amount of time.  With 4000 training examples and 1001 test examples,
we have 4 million \(l_2\) norm calculations, each involving 784 subtractions and multiplications.
Granted, this is written in Python and would be faster in a different language but it is
worth mentioning.  There are a few ways I can think of which may speed this up.  Calculating
the square root at the end of each norm calculation is unnecessary so that can be done away
with.  We could also compress each image, taking the average pixel value of blocks of 4
pixels to reduce the size from 28x28 to 14x14 which would significantly reduce the amount
of computations.  On top of this we could preprocess the training set.  This might
involve locating similar subgroups within each label group and averaging their pixel values, 
leaving only one representative image from each subgroup.  This preprocessing step 
would be computationally expensive but if it left you would a fairly small but representative
dataset, the benefits would certainly outweigh the initial cost.
</p>